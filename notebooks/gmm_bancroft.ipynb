{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering of weather data to define weather event profiles and identify their relevant characteristics\n",
    "Notebook containing program code and data for the bachelor thesis of Julian Erath<br>\n",
    "Submitted 08th of Mai 2023<br><br>\n",
    "\n",
    "\n",
    "##### Supervisors:\n",
    "Zhangziman Song, IBM Data Scientist for AI Applications<br>\n",
    "Harini Srinivasan, IBM Manager for B2B DS & AI Applications <br>\n",
    "Julia Wiegel, IBM Solutions Engineer & Meteorologist<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "0. Introduction <br>\n",
    "1. Imports, functions and options<br>\n",
    "2. Data<br>\n",
    "   1. Loading data<br>\n",
    "   2. Inspect data\n",
    "   3. Preparation\n",
    "     - cleaning data\n",
    "     - Removing NaN values with neighbor values\n",
    "3. Regular clustering model\n",
    "   1. Defining k number of clusters <br>\n",
    "   2. Clustering algorithm <br>\n",
    "   3. Evaluation metrices <br>\n",
    "   4. Data visualisation and results<br>\n",
    "4. Cascaded clustering model\n",
    "   1. First step clustering\n",
    "   2. Second step clustering\n",
    "5. PCA clustering model\n",
    "   1. Generate principal components \n",
    "   2. Analysis and results\n",
    "   3. Inspect relevance of features\n",
    "6. Seasons clustering model\n",
    "   1. Defining k number of clusters <br>\n",
    "   2. Clustering algorithm <br>\n",
    "   3. Evaluation metrices <br>\n",
    "   4. Data visualisation and results<br>\n",
    "7. Create weather event profiles\n",
    "   1. Map storm to storm2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports, functions and options<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from matplotlib import rcParams\n",
    "import warnings\n",
    "from matplotlib.patches import Circle, RegularPolygon\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.projections.polar import PolarAxes\n",
    "from matplotlib.projections import register_projection\n",
    "from matplotlib.spines import Spine\n",
    "from matplotlib.transforms import Affine2D\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "# Dataset\n",
    "from sklearn import datasets\n",
    "# Dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "# Modeling\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import DBSCAN\n",
    "#Feature Engineering / Standardization\n",
    "from scipy.stats import zscore\n",
    "from sklearn import preprocessing\n",
    "# Evaluation metrices\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "import time\n",
    "import os, psutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from resource import *\n",
    "import datetime\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "rcParams['figure.figsize'] = 20,15\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# functions\n",
    "# evaluation \n",
    "def eval_metrics(X):\n",
    "    silhouette_score_lst = []\n",
    "    elbow_score_lst = []\n",
    "    n_cluster = range(2, 20)\n",
    "    for i in n_cluster:\n",
    "        model = GaussianMixture(n_components = i, random_state = 42, reg_covar=1e-3)\n",
    "        gmm_pred = model.fit_predict(X)\n",
    "        elbow_score_lst.append(model.score(X))\n",
    "        print(i, \" cluster silhouette score: \", silhouette_score(X, gmm_pred))\n",
    "        silhouette_score_lst.append(silhouette_score(X, gmm_pred))\n",
    "    eval = pd.DataFrame(index=pd.Series(n_cluster, name='k')) \n",
    "    eval['elbow'] = elbow_score_lst\n",
    "    eval['silhouette'] = silhouette_score_lst   \n",
    "    return eval\n",
    "\n",
    "def plot_eval_metrics(eval):\n",
    "    f, axes = plt.subplots(1,2, figsize=[8,4], constrained_layout = True)\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    eval['elbow'].plot(ax=axes[0], title=\"Elbow score\")\n",
    "    axes[0].set_xlabel('Number of clusters')\n",
    "    axes[0].set_ylabel(\"Elbow score\")\n",
    "\n",
    "    eval['silhouette'].plot(ax=axes[1], title=\"Silhouette score\")\n",
    "    axes[1].set_xlabel('Number of clusters')\n",
    "    axes[1].set_ylabel(\"Silhouette score\")\n",
    "    return f\n",
    "    \n",
    "def plot_cluster_feature(df, label_col):\n",
    "    feature_show_lst = ['avg_temp','min_wet_bulb_temp','avg_dewpoint','avg_temp_change','avg_windspd','max_windgust',\n",
    "    'avg_winddir','max_cumulative_precip','max_snow_density_6',\n",
    "    'max_cumulative_snow','max_cumulative_ice','avg_pressure_change']\n",
    "\n",
    "    f, axes = plt.subplots(4, 3, constrained_layout = True, figsize=[20,15])\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    for i in df[label_col].unique():\n",
    "        df_cluster_distogram = df.loc[df[label_col] == i]\n",
    "        axes = axes.ravel()\n",
    "        num_label = df_cluster_distogram[label_col].value_counts()[i]\n",
    "        for j, feature in enumerate(feature_show_lst):\n",
    "            sns.distplot(df_cluster_distogram[feature], label=f'C{i};{num_label}', ax=axes[j])\n",
    "    axes[0].set(title=\"Avg temperature in Kelvin\", xlim=(230, 310), xlabel=\"\")\n",
    "    axes[0].legend(loc=\"best\")\n",
    "    axes[1].set(title=\"Min wet bulb temperature in Kelvin\", xlim=(230, 310), xlabel=\"\")\n",
    "    axes[2].set(title=\"Avg dewpoint in Kelvin\", xlim=(230, 310), xlabel=\"\")\n",
    "    axes[3].set(title=\"Avg temperature change in Kelvin\", xlim=(-1, 1), xlabel=\"\")\n",
    "    axes[4].set(title=\"Avg wind speed in m/s\", xlim=(0, 10), xlabel=\"\")\n",
    "    axes[5].set(title=\"Max windgust in m/s\", xlim=(0, 30), xlabel=\"\")\n",
    "    axes[6].set(title=\"Avg winddirection in degrees\", xlim=(0, 360), xlabel=\"\")\n",
    "    axes[7].set(title=\"Max cumulative precipitation in mm\", xlim=(0, 100), ylim=(0, 0.1), xlabel=\"\")\n",
    "    axes[8].set(title=\"Max snow density (when snow exceeds 6.35mm in kg/m^3)\", xlim=(0, 800), ylim=(0, 0.01), xlabel=\"\")\n",
    "    axes[9].set(title=\"Max cumulative snow accretion in mm\", xlim=(0, 1000), ylim=(0, 0.009), xlabel=\"\")\n",
    "    axes[10].set(title=\"Max cumulative ice accretion in mm\", xlim=(0, 10), ylim=(0, 0.8), xlabel=\"\")\n",
    "    axes[11].set(title=\"Avg pressure change in Pa\",  xlim=(-200, 200), xlabel=\"\")  \n",
    "    return f\n",
    "    \n",
    "\n",
    "#Code mit Änderungen übernommen aus: https://matplotlib.org/stable/gallery/specialty_plots/radar_chart.html abgerufen am 19.04.2023 09:47 Uhr\n",
    "def radar_factory(num_vars, frame='circle'):\n",
    "    # calculate evenly-spaced axis angles\n",
    "    theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n",
    "\n",
    "    class RadarTransform(PolarAxes.PolarTransform):\n",
    "\n",
    "        def transform_path_non_affine(self, path):\n",
    "            # Paths with non-unit interpolation steps correspond to gridlines,\n",
    "            # in which case we force interpolation (to defeat PolarTransform's\n",
    "            # autoconversion to circular arcs).\n",
    "            if path._interpolation_steps > 1:\n",
    "                path = path.interpolated(num_vars)\n",
    "            return Path(self.transform(path.vertices), path.codes)\n",
    "\n",
    "    class RadarAxes(PolarAxes):\n",
    "\n",
    "        name = 'radar'\n",
    "        PolarTransform = RadarTransform\n",
    "\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            # rotate plot such that the first axis is at the top\n",
    "            self.set_theta_zero_location('N')\n",
    "\n",
    "        def fill(self, *args, closed=True, **kwargs):\n",
    "            \"\"\"Override fill so that line is closed by default\"\"\"\n",
    "            return super().fill(closed=closed, *args, **kwargs)\n",
    "\n",
    "        def plot(self, *args, **kwargs):\n",
    "            \"\"\"Override plot so that line is closed by default\"\"\"\n",
    "            lines = super().plot(*args, **kwargs)\n",
    "            for line in lines:\n",
    "                self._close_line(line)\n",
    "\n",
    "        def _close_line(self, line):\n",
    "            x, y = line.get_data()\n",
    "            # FIXME: markers at x[0], y[0] get doubled-up\n",
    "            if x[0] != x[-1]:\n",
    "                x = np.append(x, x[0])\n",
    "                y = np.append(y, y[0])\n",
    "                line.set_data(x, y)\n",
    "\n",
    "        def set_varlabels(self, labels):\n",
    "            self.set_thetagrids(np.degrees(theta), labels)\n",
    "\n",
    "        def _gen_axes_patch(self):\n",
    "            # The Axes patch must be centered at (0.5, 0.5) and of radius 0.5\n",
    "            # in axes coordinates.\n",
    "            if frame == 'circle':\n",
    "                return Circle((0.5, 0.5), 0.5)\n",
    "            elif frame == 'polygon':\n",
    "                return RegularPolygon((0.5, 0.5), num_vars,\n",
    "                                      radius=.5, edgecolor=\"k\")\n",
    "            else:\n",
    "                raise ValueError(\"Unknown value for 'frame': %s\" % frame)\n",
    "\n",
    "        def _gen_axes_spines(self):\n",
    "            if frame == 'circle':\n",
    "                return super()._gen_axes_spines()\n",
    "            elif frame == 'polygon':\n",
    "                # spine_type must be 'left'/'right'/'top'/'bottom'/'circle'.\n",
    "                spine = Spine(axes=self,\n",
    "                              spine_type='circle',\n",
    "                              path=Path.unit_regular_polygon(num_vars))\n",
    "                # unit_regular_polygon gives a polygon of radius 1 centered at\n",
    "                # (0, 0) but we want a polygon of radius 0.5 centered at (0.5,\n",
    "                # 0.5) in axes coordinates.\n",
    "                spine.set_transform(Affine2D().scale(.5).translate(.5, .5)\n",
    "                                    + self.transAxes)\n",
    "                return {'polar': spine}\n",
    "            else:\n",
    "                raise ValueError(\"Unknown value for 'frame': %s\" % frame)\n",
    "\n",
    "    register_projection(RadarAxes)\n",
    "    return theta \n",
    "\n",
    "# Function to find all the local maxima and minima in the given array arr[]\n",
    "# Code mit Änderungen übernommen aus https://www.geeksforgeeks.org/find-indices-of-all-local-maxima-and-local-minima-in-an-array/ abgerufen am 19.04.2023 09:47 Uhr\n",
    "def findLocalMaximaMinima(n, arr):\n",
    "    mx = []\n",
    "    mn = []\n",
    "    if(arr[0] > arr[1]):\n",
    "        mx.append(0)\n",
    "    elif(arr[0] < arr[1]):\n",
    "        mn.append(0)\n",
    "    for i in range(1, n-1):\n",
    "        if(arr[i-1] > arr[i] < arr[i + 1]):\n",
    "            mn.append(i)\n",
    "        elif(arr[i-1] < arr[i] > arr[i + 1]):\n",
    "            mx.append(i)\n",
    "    if(arr[-1] > arr[-2]):\n",
    "        mx.append(n-1)\n",
    "    elif(arr[-1] < arr[-2]):\n",
    "        mn.append(n-1)\n",
    "    if(len(mx) > 0):\n",
    "        return(mx)\n",
    "    else:\n",
    "        print(\"There are no points of\"\\\n",
    "            \" Local maxima.\")\n",
    "    if(len(mn) > 0):\n",
    "        return(mn)\n",
    "    else:\n",
    "        print(\"There are no points\"\\\n",
    "            \" of Local minima.\")\n",
    "\n",
    "def assignStormIdHour(datetime_series, n_hours_after):\n",
    "    \"\"\" Identify same storm happened at multiple hours and multiple locations\n",
    "    and assign a list of storm id.\n",
    "    \"\"\"\n",
    "    datetime_lst = list(datetime_series)\n",
    "    index = datetime_series.index\n",
    "    \n",
    "    # Initialize the first storm id\n",
    "    n = 0\n",
    "    # Initialize storm id list with the first storm id\n",
    "    storm_id_lst = [n]\n",
    "    \n",
    "    # This candidate will be assigned the same storm id if it is n_hours_after the last candidate in the datetime_lst\n",
    "    for i in range(1, len(datetime_lst)):\n",
    "        this = datetime_lst[i]\n",
    "        last = datetime_lst[i-1]\n",
    "        \n",
    "        ## Define last candidate's neighbour\n",
    "        # last candidate and it's right neighbour\n",
    "        neighbour_dt_start = last\n",
    "        neighbour_dt_end = last + datetime.timedelta(hours=n_hours_after)\n",
    "        \n",
    "        ## Assign storm id\n",
    "        # if this candidate is in last candidate's neighbour hours, share the same storm id; else, assign new storm id\n",
    "        if this not in pd.date_range(neighbour_dt_start, neighbour_dt_end, freq='H'):\n",
    "            n += 1   \n",
    "        # every date/loop need a storm id\n",
    "        storm_id_lst.append(n)\n",
    "    \n",
    "    return pd.Series(storm_id_lst, index = index) \n",
    "\n",
    "def create_storm_table(df, id_col, agg_level):\n",
    "    ''' Summarize storm information from id_col\n",
    "    '''\n",
    "    storm_sbs = df.groupby([id_col]+agg_level).agg({\n",
    "        'valid_datetime':['min','max'],\n",
    "        })\n",
    "    \n",
    "    storm_sbs.columns = ['storm_start_dh','storm_end_dh']\n",
    "    storm_sbs = storm_sbs.reset_index()\n",
    "    storm_sbs['duration'] = storm_sbs['storm_end_dh'] - storm_sbs['storm_start_dh']\n",
    "    storm_sbs['duration_hour'] = storm_sbs['duration'].apply(lambda x: (x.total_seconds()/3600))\n",
    "    storm_sbs = storm_sbs.rename(columns={id_col:'storm_id'})\n",
    "\n",
    "    return storm_sbs\n",
    "\n",
    "# measuring time and ressources\n",
    "startTime = time.time()\n",
    "startTimeCPU = time.process_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data\n",
    "- Loading data<br>\n",
    "- Inspect data\n",
    "- Preparation\n",
    "  - cleaning data\n",
    "  - Removing NaN values with neighbor values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "df = pd.read_csv('/Users/julianerath/Documents/Bachelor Thesis/Python/Analysis_per_region/Bancroft/feature_data_substation_Bancroft.csv')\n",
    "\n",
    "#Set new index based on run_datetime so the data is chronologially sorted\n",
    "df.sort_values(by='run_datetime', inplace = True, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating avg temp change and sin / cos of winddir\n",
    "df[\"avg_temp_change\"]=df[\"avg_temp\"].diff()\n",
    "df[\"min_temp_change\"]=df[\"min_temp\"].diff()\n",
    "df[\"max_temp_change\"]=df[\"max_temp\"].diff()\n",
    "\n",
    "df[\"avg_winddir_sin\"]=np.sin(df[\"avg_winddir\"])\n",
    "df[\"avg_winddir_cos\"]=np.cos(df[\"avg_winddir\"])\n",
    "df[\"min_winddir_sin\"]=np.sin(df[\"min_winddir\"])\n",
    "df[\"min_winddir_cos\"]=np.cos(df[\"min_winddir\"])\n",
    "df[\"max_winddir_sin\"]=np.sin(df[\"max_winddir\"])\n",
    "df[\"max_winddir_cos\"]=np.cos(df[\"max_winddir\"])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN with previous valid value or drop rows since enough data is available\n",
    "# df = df.fillna(method='ffill')\n",
    "# df = df.fillna(0)\n",
    "\n",
    "\n",
    "# only use one temp feature or temp change feature\n",
    "feature_lst = ['max_windgust','avg_temp','max_snow_density_6', 'max_cumulative_precip',\n",
    "'max_cumulative_ice','max_cumulative_snow','avg_windspd','avg_winddir_sin','avg_winddir_cos']\n",
    "# 'avg_dewpoint','avg_temp', 'min_wet_bulb_temp', 'avg_temp_change', 'avg_pressure_change\n",
    "\n",
    "df = df.dropna(subset=feature_lst, how='any')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moving average over a week\n",
    "df['MA_max_windgust_week'] = df.max_windgust.rolling(168, min_periods=1).mean()\n",
    "df['MA_min_wet_bulb_temp_week'] = df.min_wet_bulb_temp.rolling(168, min_periods=1).mean()\n",
    "df['MA_max_snow_density_6_week'] = df.max_snow_density_6.rolling(168, min_periods=1).mean()\n",
    "df['MA_max_cumulative_precip_week'] = df.max_cumulative_precip.rolling(168, min_periods=1).mean()\n",
    "df['MA_max_cumulative_ice_week'] = df.max_cumulative_ice.rolling(168, min_periods=1).mean()\n",
    "df['MA_max_cumulative_snow_week'] = df.max_cumulative_snow.rolling(168, min_periods=1).mean()\n",
    "df['MA_avg_windspd_week'] = df.avg_windspd.rolling(168, min_periods=1).mean()\n",
    "df['MA_avg_pressure_change_week'] = df.avg_pressure_change.rolling(168, min_periods=1).mean()\n",
    "df['MA_avg_temp_week'] = df.avg_temp.rolling(168, min_periods=1).mean()\n",
    "df['MA_avg_dewpoint_week'] = df.avg_dewpoint.rolling(168, min_periods=1).mean()\n",
    "df['MA_avg_winddir_week'] = df.avg_winddir.rolling(168, min_periods=1).mean()\n",
    "df['MA_avg_temp_change_week'] = df.avg_temp_change.rolling(168, min_periods=1).mean()\n",
    "\n",
    "#Moving average over a month\n",
    "df['MA_max_windgust_month'] = df.max_windgust.rolling(720, min_periods=1).mean()\n",
    "df['MA_min_wet_bulb_temp_month'] = df.min_wet_bulb_temp.rolling(720, min_periods=1).mean()\n",
    "df['MA_max_snow_density_6_month'] = df.max_snow_density_6.rolling(720, min_periods=1).mean()\n",
    "df['MA_max_cumulative_precip_month'] = df.max_cumulative_precip.rolling(720, min_periods=1).mean()\n",
    "df['MA_max_cumulative_ice_month'] = df.max_cumulative_ice.rolling(720, min_periods=1).mean()\n",
    "df['MA_max_cumulative_snow_month'] = df.max_cumulative_snow.rolling(720, min_periods=1).mean()\n",
    "df['MA_avg_windspd_month'] = df.avg_windspd.rolling(720, min_periods=1).mean()\n",
    "df['MA_avg_pressure_change_month'] = df.avg_pressure_change.rolling(720, min_periods=1).mean()\n",
    "df['MA_avg_temp_month'] = df.avg_temp.rolling(720, min_periods=1).mean()\n",
    "df['MA_avg_dewpoint_month'] = df.avg_dewpoint.rolling(720, min_periods=1).mean()\n",
    "df['MA_avg_winddir_month'] = df.avg_winddir.rolling(720, min_periods=1).mean()\n",
    "df['MA_avg_temp_change_month'] = df.avg_temp_change.rolling(720, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the data\n",
    "fig, axes = plt.subplots(4, 3)\n",
    "\n",
    "axes[0, 0].plot(df.run_datetime, df.avg_temp)\n",
    "axes[0, 0].plot(df.run_datetime, df.MA_avg_temp_week)\n",
    "axes[0, 0].plot(df.run_datetime, df.MA_avg_temp_month)\n",
    "axes[0, 0].set_title('Bancroft avg_temp')\n",
    "axes[0, 0].set_ylabel(\"Kelvin\")\n",
    "axes[0, 0].get_xaxis().set_visible(False)\n",
    "\n",
    "axes[0, 1].plot(df.run_datetime, df.min_wet_bulb_temp)\n",
    "axes[0, 1].plot(df.run_datetime, df.MA_min_wet_bulb_temp_week)\n",
    "axes[0, 1].plot(df.run_datetime, df.MA_min_wet_bulb_temp_month)\n",
    "axes[0, 1].set_title('Bancroft min_wet_bulb_temp')\n",
    "axes[0, 1].set_ylabel(\"Kelvin\")\n",
    "axes[0, 1].get_xaxis().set_visible(False)\n",
    "\n",
    "axes[0, 2].plot(df.run_datetime, df.avg_dewpoint)\n",
    "axes[0, 2].plot(df.run_datetime, df.MA_avg_dewpoint_week)\n",
    "axes[0, 2].plot(df.run_datetime, df.MA_avg_dewpoint_month)\n",
    "axes[0, 2].set_title('Bancroft avg_dewpoint')\n",
    "axes[0, 2].set_ylabel(\"Kelvin\")\n",
    "axes[0, 2].get_xaxis().set_visible(False)\n",
    "\n",
    "axes[1, 0].plot(df.run_datetime, df.avg_temp_change)\n",
    "axes[1, 0].plot(df.run_datetime, df.MA_avg_temp_change_week)\n",
    "axes[1, 0].plot(df.run_datetime, df.MA_avg_temp_change_month)\n",
    "axes[1, 0].set_title('Bancroft avg_temp_change')\n",
    "axes[1, 0].set_ylabel(\"Kelvin\")\n",
    "axes[1, 0].get_xaxis().set_visible(False)\n",
    "\n",
    "axes[1, 1].plot(df.run_datetime, df.avg_windspd)\n",
    "axes[1, 1].plot(df.run_datetime, df.MA_avg_windspd_week)\n",
    "axes[1, 1].plot(df.run_datetime, df.MA_avg_windspd_month)\n",
    "axes[1, 1].set_title('Bancroft avg_windspd')\n",
    "axes[1, 1].set_ylabel(\"m/s\")\n",
    "axes[1, 1].get_xaxis().set_visible(False)\n",
    "\n",
    "axes[1, 2].plot(df.run_datetime, df.max_windgust)\n",
    "axes[1, 2].plot(df.run_datetime, df.MA_max_windgust_week)\n",
    "axes[1, 2].plot(df.run_datetime, df.MA_max_windgust_month)\n",
    "axes[1, 2].set_title('Bancroft max_windgust')\n",
    "axes[1, 2].set_ylabel(\"m/s\")\n",
    "axes[1, 2].get_xaxis().set_visible(False)\n",
    "\n",
    "axes[2, 0].plot(df.run_datetime, df.avg_winddir)\n",
    "axes[2, 0].plot(df.run_datetime, df.MA_avg_winddir_week)\n",
    "axes[2, 0].plot(df.run_datetime, df.MA_avg_winddir_month)\n",
    "axes[2, 0].set_title('Bancroft avg_winddir')\n",
    "axes[2, 0].set_ylabel(\"degrees\")\n",
    "axes[2, 0].get_xaxis().set_visible(False)\n",
    "\n",
    "axes[2, 1].plot(df.run_datetime, df.max_cumulative_precip)\n",
    "axes[2, 1].plot(df.run_datetime, df.MA_max_cumulative_precip_week)\n",
    "axes[2, 1].plot(df.run_datetime, df.MA_max_cumulative_precip_month)\n",
    "axes[2, 1].set_title('Bancroft max_cumulative_precip')\n",
    "axes[2, 1].set_ylabel(\"mm\")\n",
    "axes[2, 1].get_xaxis().set_visible(False)\n",
    "\n",
    "axes[2, 2].plot(df.run_datetime, df.max_snow_density_6)\n",
    "axes[2, 2].plot(df.run_datetime, df.MA_max_snow_density_6_week)\n",
    "axes[2, 2].plot(df.run_datetime, df.MA_max_snow_density_6_month)\n",
    "axes[2, 2].set_title('Bancroft max_snow_density_6')\n",
    "axes[2, 2].set_ylabel(\"kg/m^3\")\n",
    "axes[2, 2].get_xaxis().set_visible(False)\n",
    "\n",
    "axes[3, 0].plot(df.run_datetime, df.max_cumulative_snow)\n",
    "axes[3, 0].plot(df.run_datetime, df.MA_max_cumulative_snow_week)\n",
    "axes[3, 0].plot(df.run_datetime, df.MA_max_cumulative_snow_month)\n",
    "axes[3, 0].set_title('Bancroft max_cumulative_snow')\n",
    "axes[3, 0].set_ylabel(\"mm\")\n",
    "axes[3, 0].get_xaxis().set_visible(False)\n",
    "\n",
    "axes[3, 1].plot(df.run_datetime, df.max_cumulative_ice)\n",
    "axes[3, 1].plot(df.run_datetime, df.MA_max_cumulative_ice_week)\n",
    "axes[3, 1].plot(df.run_datetime, df.MA_max_cumulative_ice_month)\n",
    "axes[3, 1].set_title('Bancroft max_cumulative_ice')\n",
    "axes[3, 1].set_ylabel(\"mm\")\n",
    "axes[3, 1].get_xaxis().set_visible(False)\n",
    "\n",
    "axes[3, 2].plot(df.run_datetime, df.avg_pressure_change)\n",
    "axes[3, 2].plot(df.run_datetime, df.MA_avg_pressure_change_week)\n",
    "axes[3, 2].plot(df.run_datetime, df.MA_avg_pressure_change_month)\n",
    "axes[3, 2].set_title('Bancroft avg_pressure_change')\n",
    "axes[3, 2].set_ylabel(\"Pa\")\n",
    "axes[3, 2].get_xaxis().set_visible(False)\n",
    "\n",
    "plt.suptitle('Bancroft weather parameters time series')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "plt.savefig('Bancroft weather parameters time series.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/Users/julianerath/Documents/Bachelor Thesis/Python/Data/ERA5_features_20150715-20221230/feature_data_substation_Bancroft.csv', parse_dates=['valid_datetime'], index_col=0)\n",
    "df_normalized = df[['max_windgust','max_snow_density_6', 'max_cumulative_precip',\n",
    "'max_cumulative_ice','max_cumulative_snow','avg_windspd','avg_winddir_sin','avg_winddir_cos',\n",
    "'avg_dewpoint','avg_temp', 'min_wet_bulb_temp', 'avg_temp_change', 'avg_pressure_change']]\n",
    "\n",
    "df_normalized = pd.DataFrame(preprocessing.StandardScaler().fit_transform(df_normalized),columns=df_normalized.columns)\n",
    "\n",
    "\n",
    "\n",
    "df = df.dropna(subset=feature_lst, how='any')\n",
    "X = pd.DataFrame(preprocessing.StandardScaler().fit_transform(df[feature_lst]),columns=feature_lst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regular clustering model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Defining k number of clusters <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime_regular_clustering_model = time.time()\n",
    "startTimeCPU_regular_clustering_model = time.process_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_regular_clustering_model = eval_metrics(X)\n",
    "eval_regular_clustering_model.to_csv('eval_regular_clustering_model.csv')\n",
    "eval_regular_clustering_model = pd.read_csv('eval_regular_clustering_model.csv', index_col=0)\n",
    "plot_eval_metrics(eval_regular_clustering_model).savefig('Bancroft elbow and silhouette score regular clustering model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_maxima = findLocalMaximaMinima(18, eval_regular_clustering_model.silhouette.values.tolist())\n",
    "second_maximum = all_maxima[1]\n",
    "k0 = second_maximum+2\n",
    "print(\"The optimal number of k clusters for the regular clustering model is: \", k0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   3.2. Clustering algorithm <br>\n",
    "###   3.3. Evaluation metrices <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_regular_clustering_model = GaussianMixture(n_components = k0, random_state = 42, reg_covar=1e-3)\n",
    "labels_regular_clustering_mode = gmm_regular_clustering_model.fit_predict(X)\n",
    "print(\"Model score: \", gmm_regular_clustering_model.score(X), \"\\nSilhouette score: \", silhouette_score(X, labels_regular_clustering_mode)) \n",
    "\n",
    "df['label0'] = pd.Series(labels_regular_clustering_mode, index=df.index)\n",
    "df_normalized['label0'] = pd.Series(labels_regular_clustering_mode, index=df.index)\n",
    "\n",
    "display(df['label0'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Data visualisation and results<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_radar_charts_regular_clustering_model = df_normalized[['max_windgust', 'min_wet_bulb_temp', 'max_snow_density_6', \n",
    "'max_cumulative_precip', 'max_cumulative_ice', 'max_cumulative_snow', 'avg_windspd', 'avg_pressure_change', 'avg_temp', \n",
    "'avg_dewpoint', 'avg_winddir_sin','avg_winddir_cos', 'avg_temp_change','label0']]\n",
    "\n",
    "df_cluster_groups_mean_regular_clustering_model = df_radar_charts_regular_clustering_model.groupby(['label0'],as_index=False).mean()\n",
    "df_cluster_groups_mean_regular_clustering_model = df_cluster_groups_mean_regular_clustering_model.drop(\"label0\", axis=1)\n",
    "\n",
    "df_cluster_groups_max_regular_clustering_model = df_radar_charts_regular_clustering_model.groupby(['label0'],as_index=False).max()\n",
    "df_cluster_groups_max_regular_clustering_model = df_cluster_groups_max_regular_clustering_model.drop('label0', axis=1)\n",
    "\n",
    "df_cluster_groups_min_regular_clustering_model = df_radar_charts_regular_clustering_model.groupby(['label0'],as_index=False).min()\n",
    "df_cluster_groups_min_regular_clustering_model = df_cluster_groups_min_regular_clustering_model.drop('label0', axis=1)\n",
    "\n",
    "df_cluster_groups_median_regular_clustering_model = df_radar_charts_regular_clustering_model.groupby(['label0'],as_index=False).median()\n",
    "df_cluster_groups_median_regular_clustering_model = df_cluster_groups_median_regular_clustering_model.drop('label0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_data():\n",
    "    for i in range(0, 6):\n",
    "                data = [\n",
    "                    ['max_windgust', 'min_wet_bulb_temp', 'max_snow_density_6', 'max_cumulative_precip', \n",
    "                    'max_cumulative_ice', 'max_cumulative_snow', 'avg_windspd', 'avg_pressure_change', \n",
    "                    'avg_temp', 'avg_dewpoint', 'avg_winddir_sin','avg_winddir_cos', 'avg_temp_change'],\n",
    "                    ('Mean', [\n",
    "                        df_cluster_groups_mean_regular_clustering_model.iloc[0],\n",
    "                        df_cluster_groups_mean_regular_clustering_model.iloc[1],\n",
    "                        df_cluster_groups_mean_regular_clustering_model.iloc[2],\n",
    "                        df_cluster_groups_mean_regular_clustering_model.iloc[3],\n",
    "                        df_cluster_groups_mean_regular_clustering_model.iloc[4],\n",
    "                        df_cluster_groups_mean_regular_clustering_model.iloc[5],\n",
    "                    ]),\n",
    "                    ('Max', [\n",
    "                        df_cluster_groups_max_regular_clustering_model.iloc[0],\n",
    "                        df_cluster_groups_max_regular_clustering_model.iloc[1],\n",
    "                        df_cluster_groups_max_regular_clustering_model.iloc[2],\n",
    "                        df_cluster_groups_max_regular_clustering_model.iloc[3],\n",
    "                        df_cluster_groups_max_regular_clustering_model.iloc[4],\n",
    "                        df_cluster_groups_max_regular_clustering_model.iloc[5],\n",
    "                        ]),\n",
    "                    ('Min', [\n",
    "                        df_cluster_groups_min_regular_clustering_model.iloc[0],\n",
    "                        df_cluster_groups_min_regular_clustering_model.iloc[1],\n",
    "                        df_cluster_groups_min_regular_clustering_model.iloc[2],\n",
    "                        df_cluster_groups_min_regular_clustering_model.iloc[3],\n",
    "                        df_cluster_groups_min_regular_clustering_model.iloc[4],\n",
    "                        df_cluster_groups_min_regular_clustering_model.iloc[5],\n",
    "                        ]),\n",
    "                    ('Median', [\n",
    "                        df_cluster_groups_median_regular_clustering_model.iloc[0],\n",
    "                        df_cluster_groups_median_regular_clustering_model.iloc[1],\n",
    "                        df_cluster_groups_median_regular_clustering_model.iloc[2],\n",
    "                        df_cluster_groups_median_regular_clustering_model.iloc[3],\n",
    "                        df_cluster_groups_median_regular_clustering_model.iloc[4],\n",
    "                        df_cluster_groups_median_regular_clustering_model.iloc[5],\n",
    "                        ])\n",
    "                ]\n",
    "    return data\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    theta = radar_factory(13, frame='polygon')\n",
    "\n",
    "    data = example_data()\n",
    "    spoke_labels = data.pop(0)\n",
    "\n",
    "    fig, axs = plt.subplots(figsize=(14, 9), nrows=2, ncols=2,\n",
    "                            subplot_kw=dict(projection='radar'))\n",
    "    fig.subplots_adjust(wspace=0.20, hspace=0.5, top=0.85, bottom=0.05)\n",
    "\n",
    "    colors = ['b', 'r', 'g', 'm', 'y', 'w']\n",
    "    # Plot the four cases from the example data on separate axes\n",
    "    for ax, (title, case_data) in zip(axs.flat, data):\n",
    "        ax.set_yticks([-10, -5, 0, 5, 10], [\"-10\",\"-5\",\"0\",\"5\",\"10\"], color=\"grey\", size=7)\n",
    "        ax.set_ylim(-10,10)\n",
    "        ax.set_rgrids([-10, -5, 0, 5, 10])\n",
    "        ax.set_title(title, weight='bold', size='medium', position=(0.5, 1.1),\n",
    "                     horizontalalignment='center', verticalalignment='center')\n",
    "        for d, color in zip(case_data, colors):\n",
    "            ax.plot(theta, d, color=color)\n",
    "            ax.fill(theta, d, facecolor=color, alpha=0.25, label='_nolegend_')\n",
    "        ax.set_varlabels(spoke_labels)\n",
    "\n",
    "    # add legend relative to top-left plot\n",
    "    cluster_legend_names = {}\n",
    "    for i in range(0, k0):\n",
    "        cluster_legend_names.update({\"num_label_%s\" % (i): \"C%s %s\" % (i, df_radar_charts_regular_clustering_model['label0'].value_counts()[i])})\n",
    "\n",
    "    labels = tuple(cluster_legend_names.values())\n",
    "    legend = axs[0, 0].legend(labels, loc=(1.1, 1),\n",
    "                              labelspacing=0.1, fontsize='small')\n",
    "\n",
    "    fig.text(0.5, 0.965, 'Bancroft weather clusters radar charts regular clustering model',\n",
    "             horizontalalignment='center', color='black', weight='bold',\n",
    "             size='large')\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig('Bancroft weather clusters radar charts regular clustering model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[feature_lst+['label0']].groupby('label0').describe())\n",
    "df[feature_lst+['label0']].groupby('label0').describe().to_csv(\"Regular clustering model statistics Bancroft GaussianMixture.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.dropna(subset=['label0'], how='any').sort_values('label0')\n",
    "data = data.query(\"label0 in [0,1,2,3]\")\n",
    "plot_cluster_feature(data, 'label0').savefig(f'Bancroft weather clusters distograms regular method cluster 0 to 3.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.dropna(subset=['label0'], how='any').sort_values('label0')\n",
    "data = data.query(\"label0 in [4,5,6,7]\")\n",
    "plot_cluster_feature(data, 'label0').savefig(f'Bancroft weather clusters distograms regular method cluster 4 to 7.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endTime_regular_clustering_model = time.time()\n",
    "elapsedTime_regular_clustering_model = endTime_regular_clustering_model - startTime_regular_clustering_model\n",
    "print(\"Execution time: \", elapsedTime_regular_clustering_model, \" seconds\")\n",
    "\n",
    "endTimeCPU_regular_clustering_model = time.process_time()\n",
    "res_regular_clustering_model =  endTimeCPU_regular_clustering_model - startTimeCPU_regular_clustering_model\n",
    "print(\"CPU Execution time: \", res_regular_clustering_model, \" seconds\")\n",
    "\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "print(\"used non-swapped physical memory (in kiloBytes)\", process.memory_info().rss, \" kiloBytes\")  \n",
    "\n",
    "print(getrusage(RUSAGE_SELF))\n",
    "print(getrusage(RUSAGE_SELF).ru_maxrss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cascaded clustering model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime_cascaded_clustering_model = time.time()\n",
    "startTimeCPU_cascaded_clustering_model = time.process_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. First step clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval1 = eval_metrics(X)\n",
    "eval1.to_csv('eval1.csv')\n",
    "eval1 = pd.read_csv('eval1.csv', index_col=0)\n",
    "plot_eval_metrics(eval1).savefig('Bancroft elbow and silhouette score cascaded clustering model step one.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = eval1['silhouette'].idxmax()\n",
    "print(\"The optimal number of k clusters for first step of cascaded clustering is: \", k1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components = k1, random_state = 42, reg_covar=1e-3)\n",
    "labels = gmm.fit_predict(X)\n",
    "\n",
    "print(\"Model score: \", gmm.score(X), \"\\nSilhouette score: \", silhouette_score(X, labels))\n",
    "df['label1'] = pd.Series(labels, index=df.index)\n",
    "storm_cluster = df['label1'].value_counts().idxmin()\n",
    "df_normalized['label1'] = pd.Series(labels, index=df.index)\n",
    "df_sub = df.query(f'label1=={storm_cluster}')\n",
    "\n",
    "display(df['label1'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df.query(f'label1=={storm_cluster}')\n",
    "df_sub.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Second step clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval2 = eval_metrics(df_sub[feature_lst])\n",
    "plot_eval_metrics(eval2).savefig('Bancroft elbow and silhouette score cascaded clustering model step two.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k2 = eval2['silhouette'].idxmax()\n",
    "# print(\"The optimal number of k clusters for the second step of cascaded clustering is: \", k2)\n",
    "\n",
    "\n",
    "all_maxima = findLocalMaximaMinima(18, eval2.silhouette.values.tolist())\n",
    "second_maximum = all_maxima[1]\n",
    "k2 = second_maximum+2\n",
    "print(\"The optimal number of k clusters for the regular clustering model is: \", k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components = k2, random_state = 42, reg_covar=1e-3)\n",
    "label2 = gmm.fit_predict(df_sub[feature_lst])\n",
    "\n",
    "print(\"Model score: \", gmm.score(df_sub[feature_lst]), \"\\nSilhouette score: \", silhouette_score(df_sub[feature_lst], label2))\n",
    "df['label2'] = pd.Series(label2, index=df_sub.index)\n",
    "df_normalized['label2'] = pd.Series(label2, index=df_sub.index)\n",
    "df['label2'] = df['label2'].dropna().map(int)\n",
    "display(df['label2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_radar_charts = df_normalized[['max_windgust', 'min_wet_bulb_temp', 'max_snow_density_6', 'max_cumulative_precip', \n",
    "                    'max_cumulative_ice', 'max_cumulative_snow', 'avg_windspd', 'avg_pressure_change', \n",
    "                    'avg_temp', 'avg_dewpoint', 'avg_winddir_sin','avg_winddir_cos', 'avg_temp_change','label1']]\n",
    "\n",
    "df_cluster_groups_mean = df_radar_charts.groupby(['label1'],as_index=False).mean()\n",
    "df_cluster_groups_mean = df_cluster_groups_mean.drop(\"label1\", axis=1)\n",
    "\n",
    "df_cluster_groups_max = df_radar_charts.groupby(['label1'],as_index=False).max()\n",
    "df_cluster_groups_max = df_cluster_groups_max.drop('label1', axis=1)\n",
    "\n",
    "df_cluster_groups_min = df_radar_charts.groupby(['label1'],as_index=False).min()\n",
    "df_cluster_groups_min = df_cluster_groups_min.drop('label1', axis=1)\n",
    "\n",
    "df_cluster_groups_median = df_radar_charts.groupby(['label1'],as_index=False).median()\n",
    "df_cluster_groups_median = df_cluster_groups_median.drop('label1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_data():\n",
    "    for i in range(0, 6):\n",
    "                data = [\n",
    "                    ['max_windgust', 'min_wet_bulb_temp', 'max_snow_density_6', 'max_cumulative_precip', \n",
    "                    'max_cumulative_ice', 'max_cumulative_snow', 'avg_windspd', 'avg_pressure_change', \n",
    "                    'avg_temp', 'avg_dewpoint', 'avg_winddir_sin','avg_winddir_cos', 'avg_temp_change'],\n",
    "                    ('Mean', [\n",
    "                        df_cluster_groups_mean.iloc[0],\n",
    "                        df_cluster_groups_mean.iloc[1],\n",
    "                    ]),\n",
    "                    ('Max', [\n",
    "                        df_cluster_groups_max.iloc[0],\n",
    "                        df_cluster_groups_max.iloc[1],\n",
    "                        ]),\n",
    "                    ('Min', [\n",
    "                        df_cluster_groups_min.iloc[0],\n",
    "                        df_cluster_groups_min.iloc[1],\n",
    "                        ]),\n",
    "                    ('Median', [\n",
    "                        df_cluster_groups_median.iloc[0],\n",
    "                        df_cluster_groups_median.iloc[1],\n",
    "                        ])\n",
    "                ]\n",
    "    return data\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    theta = radar_factory(13, frame='polygon')\n",
    "\n",
    "    data = example_data()\n",
    "    spoke_labels = data.pop(0)\n",
    "\n",
    "    fig, axs = plt.subplots(figsize=(14, 9), nrows=2, ncols=2,\n",
    "                            subplot_kw=dict(projection='radar'))\n",
    "    fig.subplots_adjust(wspace=0.20, hspace=0.5, top=0.85, bottom=0.05)\n",
    "\n",
    "    colors = ['b', 'r', 'g', 'm', 'y', 'w']\n",
    "    # Plot the four cases from the example data on separate axes\n",
    "    for ax, (title, case_data) in zip(axs.flat, data):\n",
    "        ax.set_yticks([-10, -5, 0, 5, 10], [\"-10\",\"-5\",\"0\",\"5\",\"10\"], color=\"grey\", size=7)\n",
    "        ax.set_ylim(-10,10)\n",
    "        ax.set_rgrids([-10, -5, 0, 5, 10])\n",
    "        ax.set_title(title, weight='bold', size='medium', position=(0.5, 1.1),\n",
    "                     horizontalalignment='center', verticalalignment='center')\n",
    "        for d, color in zip(case_data, colors):\n",
    "            ax.plot(theta, d, color=color)\n",
    "            ax.fill(theta, d, facecolor=color, alpha=0.25, label='_nolegend_')\n",
    "        ax.set_varlabels(spoke_labels)\n",
    "\n",
    "    # add legend relative to top-left plot\n",
    "    cluster_legend_names = {}\n",
    "    for i in range(0, k1):\n",
    "        cluster_legend_names.update({\"num_label_%s\" % (i): \"C%s %s\" % (i, df_radar_charts['label1'].value_counts()[i])})\n",
    "\n",
    "    labels = tuple(cluster_legend_names.values())\n",
    "    legend = axs[0, 0].legend(labels, loc=(1.1, 1),\n",
    "                              labelspacing=0.1, fontsize='small')\n",
    "\n",
    "    fig.text(0.5, 0.965, 'Bancroft weather clusters radar charts',\n",
    "             horizontalalignment='center', color='black', weight='bold',\n",
    "             size='large')\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig('Bancroft weather clusters radar charts cascaded clustering first step.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[feature_lst+['label1']].groupby('label1').describe())\n",
    "df[feature_lst+['label1']].groupby('label1').describe().to_csv(\"Cascaced clustering model first step statistics Bancroft GaussianMixture.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.dropna(subset=['label1'], how='any').sort_values('label1')\n",
    "data = data.query(\"label1 in [0,1,2,3,4,5,6,7,8,9,10]\")\n",
    "plot_cluster_feature(data, 'label1').savefig(f'Bancroft weather clusters distograms cascaded method step one.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_radar_charts = df_normalized[['max_windgust', 'min_wet_bulb_temp', 'max_snow_density_6', 'max_cumulative_precip', \n",
    "                    'max_cumulative_ice', 'max_cumulative_snow', 'avg_windspd', 'avg_pressure_change', \n",
    "                    'avg_temp', 'avg_dewpoint', 'avg_winddir_sin','avg_winddir_cos', 'avg_temp_change','label2']]\n",
    "\n",
    "df_cluster_groups_mean = df_radar_charts.groupby(['label2'],as_index=False).mean()\n",
    "df_cluster_groups_mean = df_cluster_groups_mean.drop(\"label2\", axis=1)\n",
    "\n",
    "df_cluster_groups_max = df_radar_charts.groupby(['label2'],as_index=False).max()\n",
    "df_cluster_groups_max = df_cluster_groups_max.drop('label2', axis=1)\n",
    "\n",
    "df_cluster_groups_min = df_radar_charts.groupby(['label2'],as_index=False).min()\n",
    "df_cluster_groups_min = df_cluster_groups_min.drop('label2', axis=1)\n",
    "\n",
    "df_cluster_groups_median = df_radar_charts.groupby(['label2'],as_index=False).median()\n",
    "df_cluster_groups_median = df_cluster_groups_median.drop('label2', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_data():\n",
    "    for i in range(0, 6):\n",
    "                data = [\n",
    "                    ['max_windgust', 'min_wet_bulb_temp', 'max_snow_density_6', 'max_cumulative_precip', \n",
    "                    'max_cumulative_ice', 'max_cumulative_snow', 'avg_windspd', 'avg_pressure_change', \n",
    "                    'avg_temp', 'avg_dewpoint', 'avg_winddir_sin','avg_winddir_cos', 'avg_temp_change'],\n",
    "                    ('Mean', [\n",
    "                        df_cluster_groups_mean.iloc[0],\n",
    "                        df_cluster_groups_mean.iloc[1],\n",
    "                        df_cluster_groups_mean.iloc[2],\n",
    "                        df_cluster_groups_mean.iloc[3],\n",
    "                        df_cluster_groups_mean.iloc[4],\n",
    "                        df_cluster_groups_mean.iloc[5],\n",
    "                    ]),\n",
    "                    ('Max', [\n",
    "                        df_cluster_groups_max.iloc[0],\n",
    "                        df_cluster_groups_max.iloc[1],\n",
    "                        df_cluster_groups_max.iloc[2],\n",
    "                        df_cluster_groups_max.iloc[3],\n",
    "                        df_cluster_groups_max.iloc[4],\n",
    "                        df_cluster_groups_max.iloc[5],\n",
    "                        ]),\n",
    "                    ('Min', [\n",
    "                        df_cluster_groups_min.iloc[0],\n",
    "                        df_cluster_groups_min.iloc[1],\n",
    "                        df_cluster_groups_min.iloc[2],\n",
    "                        df_cluster_groups_min.iloc[3],\n",
    "                        df_cluster_groups_min.iloc[4],\n",
    "                        df_cluster_groups_min.iloc[5],\n",
    "                        ]),\n",
    "                    ('Median', [\n",
    "                        df_cluster_groups_median.iloc[0],\n",
    "                        df_cluster_groups_median.iloc[1],\n",
    "                        df_cluster_groups_median.iloc[2],\n",
    "                        df_cluster_groups_median.iloc[3],\n",
    "                        df_cluster_groups_median.iloc[4],\n",
    "                        df_cluster_groups_median.iloc[5],\n",
    "                        ])\n",
    "                ]\n",
    "    return data\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    theta = radar_factory(13, frame='polygon')\n",
    "\n",
    "    data = example_data()\n",
    "    spoke_labels = data.pop(0)\n",
    "\n",
    "    fig, axs = plt.subplots(figsize=(14, 9), nrows=2, ncols=2,\n",
    "                            subplot_kw=dict(projection='radar'))\n",
    "    fig.subplots_adjust(wspace=0.20, hspace=0.5, top=0.85, bottom=0.05)\n",
    "\n",
    "    colors = ['b', 'r', 'g', 'm', 'y', 'w']\n",
    "    # Plot the four cases from the example data on separate axes\n",
    "    for ax, (title, case_data) in zip(axs.flat, data):\n",
    "        ax.set_yticks([-10, -5, 0, 5, 10], [\"-10\",\"-5\",\"0\",\"5\",\"10\"], color=\"grey\", size=7)\n",
    "        ax.set_ylim(-10,10)\n",
    "        ax.set_rgrids([-10, -5, 0, 5, 10])\n",
    "        ax.set_title(title, weight='bold', size='medium', position=(0.5, 1.1),\n",
    "                     horizontalalignment='center', verticalalignment='center')\n",
    "        for d, color in zip(case_data, colors):\n",
    "            ax.plot(theta, d, color=color)\n",
    "            ax.fill(theta, d, facecolor=color, alpha=0.25, label='_nolegend_')\n",
    "        ax.set_varlabels(spoke_labels)\n",
    "\n",
    "    # add legend relative to top-left plot\n",
    "    cluster_legend_names = {}\n",
    "    for i in range(0, k2):\n",
    "        cluster_legend_names.update({\"num_label_%s\" % (i): \"C%s %s\" % (i, df_radar_charts['label2'].value_counts()[i])})\n",
    "\n",
    "    labels = tuple(cluster_legend_names.values())\n",
    "    legend = axs[0, 0].legend(labels, loc=(1.1, 1),\n",
    "                              labelspacing=0.1, fontsize='small')\n",
    "\n",
    "    fig.text(0.5, 0.965, 'Bancroft weather clusters radar charts',\n",
    "             horizontalalignment='center', color='black', weight='bold',\n",
    "             size='large')\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig('Bancroft weather clusters radar charts cascaded clustering second step.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[feature_lst+['label2']].groupby('label2').describe())\n",
    "df[feature_lst+['label2']].groupby('label2').describe().to_csv(\"Cascaced clustering model second step statistics Bancroft GaussianMixture.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.dropna(subset=['label2'], how='any').sort_values('label2')\n",
    "data = data.query(\"label2 in [0,1,2,3]\")\n",
    "plot_cluster_feature(data, 'label2').savefig(f'Bancroft weather clusters distograms cascaded method step two (clusters 0 to 3).png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.dropna(subset=['label2'], how='any').sort_values('label2')\n",
    "data = data.query(\"label2 in [4,5,6]\")\n",
    "plot_cluster_feature(data, 'label2').savefig(f'Bancroft weather clusters distograms cascaded method step two (clusters 4 to 6).png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endTime_cascaded_clustering_model = time.time()\n",
    "elapsedTime_cascaded_clustering_model = endTime_cascaded_clustering_model - startTime_cascaded_clustering_model\n",
    "print(\"Execution time: \", elapsedTime_cascaded_clustering_model, \" seconds\")\n",
    "\n",
    "endTimeCPU_cascaded_clustering_model = time.process_time()\n",
    "res_cascaded_clustering_model =  endTimeCPU_cascaded_clustering_model - startTimeCPU_cascaded_clustering_model\n",
    "print(\"CPU Execution time: \", res_cascaded_clustering_model, \" seconds\")\n",
    "\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "print(\"used non-swapped physical memory (in kiloBytes)\", process.memory_info().rss, \" kiloBytes\")  \n",
    "\n",
    "print(getrusage(RUSAGE_SELF))\n",
    "print(getrusage(RUSAGE_SELF).ru_maxrss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PCA clustering model\n",
    "### 5.1. Generate principal components \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime_pca_clustering_model = time.time()\n",
    "startTimeCPU_pca_clustering_model = time.process_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_df_cluster = pca.fit_transform(X)\n",
    "\n",
    "principal_components_df = pd.DataFrame(data = pca_df_cluster, columns = ['principal component 0', 'principal component 1', 'principal component 2'])\n",
    "principal_components_df.to_csv(\"principal_components_df_Bancroft.csv\")\n",
    "principal_components_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Analysis and results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pca = eval_metrics(pca_df_cluster)\n",
    "plot_eval_metrics(eval_pca).savefig('Bancroft elbow and silhouette score pca clustering model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_maxima = findLocalMaximaMinima(18, eval_pca.silhouette.values.tolist())\n",
    "second_maximum = all_maxima[1]\n",
    "k3 = second_maximum+2\n",
    "print(\"The optimal number of k clusters for the pca model is: \", k3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_pca = GaussianMixture(n_components = k3, random_state = 42, reg_covar=1e-3)\n",
    "labels_pca = gmm_pca.fit_predict(principal_components_df)\n",
    "\n",
    "print(\"Model score: \", gmm_pca.score(principal_components_df), \"\\nSilhouette score: \", silhouette_score(principal_components_df, labels_pca))\n",
    "df['label3'] = pd.Series(labels_pca, index=df.index)\n",
    "df_normalized['label3'] = pd.Series(labels_pca, index=df.index)\n",
    "principal_components_df['label3'] = pd.Series(labels_pca, index=df.index)\n",
    "\n",
    "\n",
    "display(df['label3'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_components_df = principal_components_df.dropna()\n",
    "principal_components_df['label3'] = principal_components_df['label3'].astype({'label3':'int'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_components_df_mean = principal_components_df.groupby(['label3'],as_index=False).mean()\n",
    "principal_components_df_mean = principal_components_df_mean.drop('label3', axis=1)\n",
    "\n",
    "principal_components_df_max = principal_components_df.groupby(['label3'],as_index=False)[\"principal component 0\", \"principal component 1\", \"principal component 2\"].max()\n",
    "principal_components_df_max = principal_components_df_max.drop('label3', axis=1)\n",
    "\n",
    "principal_components_df_min = principal_components_df.groupby(['label3'],as_index=False)[\"principal component 0\", \"principal component 1\", \"principal component 2\"].min()\n",
    "principal_components_df_min = principal_components_df_min.drop('label3', axis=1)\n",
    "\n",
    "principal_components_df_median = principal_components_df.groupby(['label3'],as_index=False)[\"principal component 0\", \"principal component 1\", \"principal component 2\"].median()\n",
    "principal_components_df_median = principal_components_df_median.drop('label3', axis=1)\n",
    "\n",
    "\n",
    "def example_data():\n",
    "    for i in range(0, 6):\n",
    "                data = [\n",
    "        [\"principal component 0\", \"principal component 1\", \"principal component 2\"],\n",
    "        ('Mean', [\n",
    "            principal_components_df_mean.iloc[0],\n",
    "            principal_components_df_mean.iloc[1],\n",
    "            principal_components_df_mean.iloc[2],\n",
    "            principal_components_df_mean.iloc[3],\n",
    "        ]),\n",
    "        ('Max', [\n",
    "            principal_components_df_max.iloc[0],\n",
    "            principal_components_df_max.iloc[1],\n",
    "            principal_components_df_max.iloc[2],\n",
    "            principal_components_df_max.iloc[3],\n",
    "            ]),\n",
    "        ('Min', [\n",
    "            principal_components_df_min.iloc[0],\n",
    "            principal_components_df_min.iloc[1],\n",
    "            principal_components_df_min.iloc[2],\n",
    "            principal_components_df_min.iloc[3],\n",
    "            ]),\n",
    "        ('Median', [\n",
    "            principal_components_df_median.iloc[0],\n",
    "            principal_components_df_median.iloc[1],\n",
    "            principal_components_df_median.iloc[2],\n",
    "            principal_components_df_median.iloc[3],\n",
    "            ])\n",
    "    ]\n",
    "    return data\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    theta = radar_factory(3, frame='polygon')\n",
    "\n",
    "    data = example_data()\n",
    "    spoke_labels = data.pop(0)\n",
    "\n",
    "    fig, axs = plt.subplots(figsize=(14, 9), nrows=2, ncols=2,\n",
    "                            subplot_kw=dict(projection='radar'))\n",
    "    fig.subplots_adjust(wspace=0.20, hspace=0.5, top=0.85, bottom=0.05)\n",
    "\n",
    "    colors = ['b', 'r', 'g', 'm', 'y', 'w']\n",
    "    # Plot the four cases from the example data on separate axes\n",
    "    for ax, (title, case_data) in zip(axs.flat, data):\n",
    "        ax.set_yticks([-10, -5, 0, 5, 10], [\"-10\",\"-5\",\"0\",\"5\",\"10\"], color=\"grey\", size=7)\n",
    "        ax.set_ylim(-10,10)\n",
    "        ax.set_rgrids([-10, -5, 0, 5, 10])\n",
    "        ax.set_title(title, weight='bold', size='medium', position=(0.5, 1.1),\n",
    "                     horizontalalignment='center', verticalalignment='center')\n",
    "        for d, color in zip(case_data, colors):\n",
    "            ax.plot(theta, d, color=color)\n",
    "            ax.fill(theta, d, facecolor=color, alpha=0.25, label='_nolegend_')\n",
    "        ax.set_varlabels(spoke_labels)\n",
    "\n",
    "    # add legend relative to top-left plot\n",
    "    cluster_legend_names = {}\n",
    "    for i in range(0, k3):\n",
    "        cluster_legend_names.update({\"num_label_%s\" % (i): \"C%s %s\" % (i, principal_components_df['label3'].value_counts()[i])})\n",
    "\n",
    "    labels = tuple(cluster_legend_names.values())\n",
    "    legend = axs[0, 0].legend(labels, loc=(1.1, 1),\n",
    "                              labelspacing=0.1, fontsize='small')\n",
    "\n",
    "    fig.text(0.5, 0.965, 'Bancroft weather clusters radar charts',\n",
    "             horizontalalignment='center', color='black', weight='bold',\n",
    "             size='large')\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig('Bancroft weather clusters radar charts PCA.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 3, constrained_layout = True, figsize=[12,4])\n",
    "sns.set(style=\"darkgrid\")\n",
    "feature_lst_pca = ['principal component 0','principal component 1','principal component 2']\n",
    "\n",
    "for i in principal_components_df['label3'].unique():\n",
    "    df_cluster_distogram = principal_components_df.loc[principal_components_df['label3'] == i]\n",
    "    axes = axes.ravel()\n",
    "    num_label = df_cluster_distogram['label3'].value_counts()[i]\n",
    "    for j, feature in enumerate(feature_lst_pca):\n",
    "        sns.distplot(df_cluster_distogram[feature], label=f'C{i};{num_label}', ax=axes[j])\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "f.savefig(f'Bancroft weather clusters distograms pca method.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Inspect relevance of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_df_cluster)\n",
    "print(abs( pca.components_ ))\n",
    "pca_feature_relevance = pd.DataFrame(abs( pca.components_ ))\n",
    "pca_feature_relevance.to_csv(\"pca_feature_relevance_Bancroft.csv\")\n",
    "pca_feature_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = go.Scatter3d(\n",
    "    x=pca_df_cluster[:,0],\n",
    "    y = pca_df_cluster[:,1],\n",
    "    z = pca_df_cluster[:,2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=3,\n",
    "        color= principal_components_df['label3'],                \n",
    "        opacity=1\n",
    ")\n",
    "\n",
    ")\n",
    "\n",
    "dc_1 = go.Scatter3d( x = [0,pca.components_.T[0][0]],\n",
    "                     y = [0,pca.components_.T[0][1]],\n",
    "                     z = [0,pca.components_.T[0][2]],\n",
    "                     marker = dict( size = 1,\n",
    "                                    color = \"rgb(84,48,5)\"),\n",
    "                     line = dict( color = \"red\",\n",
    "                                width = 6),\n",
    "                     name = \"Var1\"\n",
    "                     )\n",
    "dc_2 = go.Scatter3d( x = [0,pca.components_.T[1][0]],\n",
    "                   y = [0,pca.components_.T[1][1]],\n",
    "                   z = [0,pca.components_.T[1][2]],\n",
    "                   marker = dict( size = 1,\n",
    "                                  color = \"rgb(84,48,5)\"),\n",
    "                   line = dict( color = \"green\",\n",
    "                                width = 6),\n",
    "                   name = \"Var2\"\n",
    "                 )\n",
    "dc_3 = go.Scatter3d( x = [0,pca.components_.T[2][0]],\n",
    "                     y = [0,pca.components_.T[2][1]],\n",
    "                     z = [0,pca.components_.T[2][2]],\n",
    "                     marker = dict( size = 1,\n",
    "                                  color = \"rgb(84,48,5)\"),\n",
    "                     line = dict( color = \"blue\",\n",
    "                                width = 6),\n",
    "                     name = \"Var3\"\n",
    "                 ) \n",
    "dc_4 = go.Scatter3d( x = [0,pca.components_.T[3][0]],\n",
    "                     y = [0,pca.components_.T[3][1]],\n",
    "                     z = [0,pca.components_.T[3][2]],\n",
    "                     marker = dict( size = 1,\n",
    "                                  color = \"rgb(84,48,5)\"),\n",
    "                     line = dict( color = \"yellow\",\n",
    "                                width = 6),\n",
    "                     name = \"Var4\"\n",
    "                   )\n",
    "\n",
    "\n",
    "data = [trace1,dc_1,dc_2,dc_3,dc_4]\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(\n",
    "        title='PC1',\n",
    "        titlefont=dict(\n",
    "           family='Courier New, monospace',\n",
    "           size=18,\n",
    "           color='#7f7f7f'\n",
    "       )\n",
    "   )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "pyo.iplot(data, filename = 'basic-line')\n",
    "\n",
    "\n",
    "#https://stackoverflow.com/questions/48385273/3d-biplot-in-plotly-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(principal_components_df, x='principal component 0', y='principal component 1', z='principal component 2',\n",
    "                    color='label3', symbol='label3', size_max=1, opacity=0.7)\n",
    "fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "fig.update_traces(marker=dict(size=2),\n",
    "                  selector=dict(mode='markers'))\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endTime_pca_clustering_model = time.time()\n",
    "elapsedTime_pca_clustering_model = endTime_pca_clustering_model - startTime_pca_clustering_model\n",
    "print(\"Execution time: \", elapsedTime_pca_clustering_model, \" seconds\")\n",
    "\n",
    "endTimeCPU_pca_clustering_model = time.process_time()\n",
    "res_pca_clustering_model =  endTimeCPU_pca_clustering_model - startTimeCPU_pca_clustering_model\n",
    "print(\"CPU Execution time: \", res_pca_clustering_model, \" seconds\")\n",
    "\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "print(\"used non-swapped physical memory (in kiloBytes)\", process.memory_info().rss, \" kiloBytes\")  \n",
    "\n",
    "print(getrusage(RUSAGE_SELF))\n",
    "print(getrusage(RUSAGE_SELF).ru_maxrss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create weather event profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naming the weather clusters\n",
    "df['wep'] = df['label2']\n",
    "df['wep'] = df['wep'].fillna('Blue sky day')\n",
    "df['wep'].mask(df['wep'] == 0 ,'Moderate snowfall', inplace=True)\n",
    "df['wep'].mask(df['wep'] == 1 ,'Heavy snowfall with accumulated snow', inplace=True)\n",
    "df['wep'].mask(df['wep'] == 2 ,'Storm with freezing rain / Heavy snow- and icestorm', inplace=True)\n",
    "df['wep'].mask(df['wep'] == 3 ,'Moderate rain', inplace=True)\n",
    "df['wep'].mask(df['wep'] == 4 ,'Snowstorm with high precipitation', inplace=True)\n",
    "df['wep'].mask(df['wep'] == 5 ,'Mild snowfall', inplace=True)\n",
    "df['wep'].mask(df['wep'] == 6 ,'Frondurchlauf / Continuous freezing rain', inplace=True)\n",
    "\n",
    "df.to_csv(\"feature_data_substation_bancroft_labelled.csv\")\n",
    "\n",
    "df[df.wep != 'Blue sky day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_storm_table(df, id_col, agg_level):\n",
    "    ''' Summarize storm information from id_col\n",
    "    '''\n",
    "    storm_sbs = df.groupby([id_col]+agg_level).agg({\n",
    "        'valid_datetime':['min','max'],\n",
    "        })\n",
    "    \n",
    "    storm_sbs.columns = ['storm_start_dh','storm_end_dh']\n",
    "    storm_sbs = storm_sbs.reset_index()\n",
    "    storm_sbs['duration'] = storm_sbs['storm_end_dh'] - storm_sbs['storm_start_dh']\n",
    "    storm_sbs['duration_hour'] = storm_sbs['duration'].apply(lambda x: (x.total_seconds()/3600))\n",
    "    storm_sbs = storm_sbs.rename(columns={id_col:'storm_id'})\n",
    "\n",
    "    return storm_sbs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign weather / storm ID by cluster label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign storm id by cluster label\n",
    "df['valid_datetime'] = pd.to_datetime(df['valid_datetime'])\n",
    "storm_id_series_lst = []\n",
    "for cluster_id in df.label2.dropna().unique():\n",
    "    cluster_id = int(cluster_id)\n",
    "    dt_series = df.query(\"label2==%s\"%cluster_id).sort_values(['valid_datetime'])['valid_datetime']\n",
    "    storm_id_series = str(cluster_id)+'_s' + assignStormIdHour(dt_series, 8).map(lambda x: str(x).zfill(3))\n",
    "    storm_id_series_lst.append(storm_id_series)\n",
    "# add storm id to df    \n",
    "df['storm_id'] = pd.concat(storm_id_series_lst)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign weather / storm ID for all weather / storm days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign storm id, all storm days\n",
    "dt_series = df.loc[~df['label2'].isnull()].sort_values(['valid_datetime'])['valid_datetime']\n",
    "df['storm_id2'] = 's' + assignStormIdHour(dt_series, 8).map(lambda x: str(x).zfill(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df['valid_datetime'].dt.year\n",
    "df['month'] = df['valid_datetime'].dt.month  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Map storm by cluster to storm_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storm = df.groupby(['year','storm_id2','storm_id','label2']).agg({\n",
    "    'valid_datetime':['min','max'],'min_temp':['min'],'max_windgust':['max'],'max_windspd':['max'],\n",
    "    'max_snow_density_6':['max'],'max_cumulative_precip':['max'],'max_cumulative_snow':['max'],'max_cumulative_ice':['max'],'wep':['max']\n",
    "    })\n",
    "\n",
    "storm.columns = ['storm_start_dh','storm_end_dh','min_temp','max_windgust','max_windspd','max_snow_density','max_cumulative_precip','max_cumulative_snow','max_cumulative_ice','wep']\n",
    "storm = storm.reset_index()\n",
    "storm['duration'] = storm['storm_end_dh'] - storm['storm_start_dh']\n",
    "storm['duration_hour'] = storm['duration'].apply(lambda x: (x.total_seconds()/3600))\n",
    "storm['label2'] = storm['label2'].map(int)\n",
    "storm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "storm.query(\"year==2022 and duration_hour>3\").to_csv(\"Weatherevents for 2022.csv\")\n",
    "storm.query(\"year==2022 and duration_hour>3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storm.query(\"year==2022 and duration_hour>3\").groupby('storm_id2').agg({'label2':list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endTime = time.time()\n",
    "elapsedTime = endTime - startTime\n",
    "print(\"Execution time: \", elapsedTime, \" seconds\")\n",
    "\n",
    "endTimeCPU = time.process_time()\n",
    "res =  endTimeCPU - startTimeCPU\n",
    "print(\"CPU Execution time: \", res, \" seconds\")\n",
    "\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "print(\"used non-swapped physical memory (in kiloBytes)\", process.memory_info().rss, \" kiloBytes\")  \n",
    "\n",
    "print(getrusage(RUSAGE_SELF))\n",
    "print(getrusage(RUSAGE_SELF).ru_maxrss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
